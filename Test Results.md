## Результаты тестирования алгоритмов по результатам обучения


№  | Algorithm     |F1 Macro AVG   | F1 Macro STD  | time, s     | PC settings                 |
---|---------------|---------------|---------------|---------------|------------------------------|
 1  | Naive Bayes   | 0.8866        |  0.045        | 1.16e-05      | Intel(R)Core(TM) i3-7100U CPU @ 2.40GHz2133MHz  6GiB|
  2 | Bert + LogReg | 0.9108        | 0.0334        | 1.28s         | 2.3 GHz Intel Core i58 ГБ 2133 MHz LPDDR3|
  3 | USE           | 0.9033        | 0.0456        | 0.0966        |                               |
  4 |BiLSTM+MaxPool | 0,899575     | 0.035594       |               | GTX 850M 2gb       |
  5 | CNN           | 0.9306         | 0.0412        |               |CPU: Intel Core i5-6200U @ 2.30GHz, 2 cores   |     
 6  | Tf-IDF +   Классический персептрон           |  0.06498     | 0.06498         |     0.000745    |               |               
 7  | Seq2CNN       | 0.8988    |               |    2.86е-5       |               |               |

В таблице представлены оценки алгоритмов в режиме кросс-валидации на датасете из 877 выражений. Использовано разделение на 10 блоков. 
## Описание реализованных алгоритмов
# Bert + LogReg
Для получения embeddings предложений использовался BERT, версия обученной модели BERT-Base, Multilingual Cased. Bert добавляет к каждому предложению специальный токен [CLS], который используется для классификации всего предложения. 
Алгоритм:
1. Получаем векторное представление каждого предложения по токену [CLS] 
2. Обучаем на них LogisticRegression(random_state=42, solver='lbfgs',multi_class='multinomial',max_iter=3000).
3. Обучение происходит по фолдам с сохранение баланса классов, StratifiedShuffleSplit(n_splits=10, random_state=42).

## USE + Google Translate
Для получения embeddings предложений использовался Use, версия обученная на английском языке. Слова переводились с русского на английский с помощью goggle translate
Алгоритм:
Получаем векторное представление каждого предложения с помощью USE
Обучаем на нихXGBoostClassifier().
Обучение происходит по фолдам с сохранение баланса классов, StratifiedShuffleSplit(n_splits=10, random_state=42).
Собственно, все.


# Mlpclassifier+TF-IDF
Классический персептрон  
Оптимальные параметры для конкретной задачи классификации
* hidden_layer_sizes=80,
* activation='relu',
* solver='adam',
* random_state=42
* Для получения вектора использовался алгоритм tf-IDF

# Convolutional Neural Network
Реализована двухслойная сверточная одномерная сеть. Первый слой сети - Embedding слой на предобученном наборе FastText Эмбеддингов для русского языка с сайта rusvictories. При переходе с первого слоя на второй используется MaxPooling. 
Алгоритм: 
1. подготовка данных. для загрузки датасета в нейронную сеть кажой предложение предварительно токенезировали и векторизовывали. 
2. Создание Embedding-матрицы для датасета
3. Формирование нейронной сети
* Embedding слой
* Одномерный слой свертки с ядром размера 5
* Maxpooling (k = 2)
* Одномерный слой свертки с ядром размера 5
* GlobalMaxPooling
* Полносвязный выходной слой с 10 нейронами (датасет разбит на 10 классов)
4. Кроссвалидация, K(fold) = 10


